---
title: "Water and Drought Plans"
author: "Rachel McKane"
date: "August 28, 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(root.dir = "  ") # Set your root directory 

rm(list=ls()) #clears variables in global environment 

getwd()#"Z:/Documents/Projects/Vanderbilt/State Water Plans/StateWaterPlans/WaterPlanProject"
```

```{r}
library(tidyverse) #includes dplyr, ggplot2, tidyr, purr, readr, and tibble
library(tm) #has a lot of text cleaning fns, like remove numbers and punctuation; incl. NLP and slam
library(wordcloud)
library(RColorBrewer)

# library("broom") #has the tidy fn to convert non-tidy objects into tidy objects
# library("SnowballC")
# library("stringr") #has str_extract fn to avoid some of the strange utf-8 text encodings
# library("tidytext") #has unnest_tokens fn; includes SnowballC; #has tidy fn to convert topics into tidy format
# library("quanteda") #searching for specific words
# library("pdftools") #reading in pdfs
```

### Read in data

```{r}
list.files(pattern = "rds", ignore.case=TRUE)
WaterDroughtPlans <- readRDS("StatePlans_WaterDrought_RawProcessed.Rds")
str(WaterDroughtPlans)

#data quality issues
WaterDroughtPlans %>%
  filter(State_Abbreviation =="PA" & Plan_Type =="WaterPlanProcessed") %>%
  select(Plan_Text) #good - previous data quality issue fixed

WaterDroughtPlans %>%
  mutate(PlanType = substr(Plan_Type,1,2)) %>% #create unique ID for waterplan vs droughtplan
  group_by(State) %>%
  count(PlanType) %>%
  filter(n !=2) #no state plans with non-2 representatives (i.e., both raw and processed are present)
```

### Explore water plans 

create fn to generate word clouds of processed text
```{r}
wordcloudRows <- function(df, row, addstopwords, figname){
  #read in text and generate corpus
  dfrow <- df$Plan_Text[row]
  text <- Corpus(VectorSource(dfrow))
  
  #clean up weird symbols
  (f_weirdos <- content_transformer(function(x, pattern) gsub(pattern, "", x)))
  
  #define all stop words (including specific ones we add, such as state names) - do first because they contain punctuation
  stpwrds <- tm::stopwords("SMART") #list of stopwords in the tm pkg for the SMART option
  addwords <- c(stpwrds, "state", "water","plan", "drought", addstopwords) 
  all_stopwords <- tolower(addwords) #make all state names/abbrevs lowercase
  rm(stpwrds,addwords) 

  #clean up text
  text_clean <- text %>%
    tm_map(f_weirdos,"[^[:alnum:] ]") %>% #gets rid of form feed (/f) and euro symbol, but not a accents
    tm_map(f_weirdos,"â") %>% #remove accents
    tm_map(removePunctuation) %>% #Remove punctuation marks
    tm_map(removeNumbers) %>% #Remove numbers  
    tm_map(tolower) %>% #Switch to lower case
    tm_map(removeWords, all_stopwords) %>% #remove stopwords; want to remove stopwords 1st because they contain punctuation
    tm_map(stripWhitespace)# %>% #Remove white space
    

  #create tdm and generate freq
  tdm <- TermDocumentMatrix(text_clean) # turn it into a term document matrix

  m <- as.matrix(tdm)
  freq <- sort(rowSums(m), decreasing = TRUE)

  #set colors
  pal <- brewer.pal(9, "BuGn")
  pal <- pal[-(1:2)]

  #print wordcloud to dashboard - adjust scale/max words as needed to make words fit on page
  #wordcloud(words = names(freq), freq = freq, min.freq = 2, random.order = FALSE,
   #         colors=pal, scale=c(4,.2), max.words=100, rot.per=.15)

  #if wanted to, save fig; else comment out
  png(figname,width=8,height=8,units="in",res=1200)#, width=400,height=3000
  layout(matrix(c(1, 2), nrow=2), heights=c(1, 4))
  par(mar=rep(0, 4))
  plot.new()
  text(x=0.5, y=0.5, figname)
  set.seed(915345) #for reproducibility purposes
  wordcloud(words = names(freq), freq = freq, min.freq = 2, random.order = FALSE,
            colors=pal, scale=c(4,.2), max.words=100, rot.per=.15)
  dev.off()
  
  #return(text_clean)
}


# #now do some special corpus cleaning...order of functions matters!
# DroughtPlans_text_clean <- WaterPlans_text %>%
#   tm_map(f_weirdos,"[^[:alnum:] ]") %>% #gets rid of form feed (/f) and euro symbol, but not a accents
#   tm_map(f_weirdos,"â") %>% #remove accents
#   tm_map(content_transformer(tolower)) %>% #Switch to lower case
#   tm_map(removeWords, all_stopwords) %>% #remove stopwords; want to remove stopwords 1st because they contain punctuation
#   tm_map(removePunctuation) %>% #Remove punctuation marks 
#   tm_map(removeNumbers) %>% #Remove numbers
```

Generate word clouds for each plan
```{r}
#which df to focus on?
WaterDroughtPlans_Raw <- WaterDroughtPlans %>% filter(!Plan_Type == "WaterPlanProcessed",
                                                      !Plan_Type =="DroughtPlanProcessed")

for (i in 1:32){#dim(WaterDroughtPlans_Raw)[1] - need to troubleshoot i=33, DE_DroughtPlanRaw
  #create figname iteratively
  figname = paste("./Figures/wordcloud",WaterDroughtPlans_Raw$State_Abbreviation[i],sep="_")
  figname = paste(figname,WaterDroughtPlans_Raw$Plan_Type[i],sep="_")
  figname = paste(figname, ".png",sep="")
  print(figname)
  
  stopwrd = c(WaterDroughtPlans_Raw$State[i],WaterDroughtPlans_Raw$State_Abbreviation[i])
  
  #print(stopwrd)
  
  wordcloudRows(WaterDroughtPlans_Raw,i,stopwrd,figname) #issue at i=33, DE_DroughtPlanRaw
  rm(i,stopwrd,figname)
}

# data quality issue - 
# WaterDroughtPlans %>% 
#   filter(State=="Delaware") %>%
#   filter(Plan_Type == "DroughtPlanRaw") %>%
#   select(Plan_Text)
```

### Total Terms with Stopwords for Water Plans

It is easier to use the quanteda package for word searching, so we start by converting our VCorpus (from the tm package) to a quanteda corpus. 

```{r}

WaterPlanCorpus <- corpus(WaterPlans_text)
total_terms_raw <- summary(WaterPlanCorpus)
total_terms_raw_words <- total_terms_raw$Tokens


```

### Word Searches for Water Plans

Now we can search for specific words. 

```{r}

#Sustainability
sust<- kwic(WaterPlanCorpus, "sustainability") 
sust<- table(sust$docname) %>% tidy()
sust<- sust %>% dplyr::rename(State = "Var1", Sustainability = "Freq")


#Climate Change
cc<- kwic(WaterPlanCorpus, c(phrase("climate change")))
cc<- table(cc$docname) %>% tidy()
cc<- cc %>% dplyr::rename(State = "Var1", Climate_Change = "Freq")

#Scarcity 
scar<- kwic(WaterPlanCorpus, "scarcity") 
scar<- table(scar$docname) %>% tidy()
scar<- scar %>% dplyr::rename(State = "Var1", Scarcity = "Freq")


#Climate Varaibility 
cv<- kwic(WaterPlanCorpus, c(phrase("climate variability")))
cv<- table(cv$docname) %>% tidy()
cv<- cv %>% dplyr::rename(State = "Var1", Climate_Variability = "Freq")


#Extreme Events
ee<- kwic(WaterPlanCorpus, c(phrase("extreme events")))
ee<- table(ee$docname) %>% tidy()
ee<- ee %>% dplyr::rename(State = "Var1", Extreme_Events = "Freq")

#Quality
qq<- kwic(WaterPlanCorpus, c(phrase("quality")))
qq<- table(qq$docname) %>% tidy()
qq<- qq %>% dplyr::rename(State = "Var1", Quality = "Freq")


```

### Cleaning up Word Data for Water Plans 

Now we can clean up our word data, standardize our findings by the total number of words in each doument, and create a table that has all of our results. 

```{r}

word_data_waterplans <- total_terms %>% left_join(cc) %>% left_join(sust) %>% 
  left_join(scar) %>% left_join(cv) %>% left_join(ee) %>% left_join(qq)

word_data_waterplans <- word_data_waterplans %>%
  mutate(Climate_Change_Percent = Climate_Change/Total_Words*100,
         Sustainability_Percent = Sustainability/Total_Words*100,
         Climate_Variabiliyt_Percent = Climate_Variability/Total_Words*100,
         Extreme_Events_Percent = Extreme_Events/Total_Words*100,
         Quality_Percent = Quality/Total_Words*100)

write.csv(word_data_waterplans,"~/word_data_waterplans.csv")

```


Now, we can do the same thing with the drought plans...


```{r}

DroughtPlans_text <- readRDS(file = "~/DroughtPlans_text.rds")


DroughtPlanCorpus <- corpus(DroughtPlans_text)
total_terms_raw <- summary(DroughtPlanCorpus)
total_terms_raw_words <- total_terms_raw$Tokens

#Sustainability
sust<- kwic(DroughtPlanCorpus, "sustainability") 
sust<- table(sust$docname) %>% tidy()
sust<- sust %>% dplyr::rename(State = "Var1", Sustainability = "Freq")


#Climate Change
cc<- kwic(DroughtPlanCorpus, c(phrase("climate change")))
cc<- table(cc$docname) %>% tidy()
cc<- cc %>% dplyr::rename(State = "Var1", Climate_Change = "Freq")

#Scarcity 
scar<- kwic(DroughtPlanCorpus, "scarcity") 
scar<- table(scar$docname) %>% tidy()
scar<- scar %>% dplyr::rename(State = "Var1", Scarcity = "Freq")


#Climate Varaibility 
cv<- kwic(DroughtPlanCorpus, c(phrase("climate variability")))
cv<- table(cv$docname) %>% tidy()
cv<- cv %>% dplyr::rename(State = "Var1", Climate_Variability = "Freq")


#Extreme Events
ee<- kwic(DroughtPlanCorpus, c(phrase("extreme events")))
ee<- table(ee$docname) %>% tidy()
ee<- ee %>% dplyr::rename(State = "Var1", Extreme_Events = "Freq")

#Quality
qq<- kwic(DroughtPlanCorpus, c(phrase("quality")))
qq<- table(qq$docname) %>% tidy()
qq<- qq %>% dplyr::rename(State = "Var1", Quality = "Freq")


word_data_droughtpans <- total_terms %>% left_join(cc) %>% left_join(sust) %>% 
  left_join(scar) %>% left_join(cv) %>% left_join(ee) %>% left_join(qq)

word_data_droughtpans  <- word_data_droughtpans  %>%
  mutate(Climate_Change_Percent = Climate_Change/Total_Words*100,
         Sustainability_Percent = Sustainability/Total_Words*100,
         Climate_Variabiliyt_Percent = Climate_Variability/Total_Words*100,
         Extreme_Events_Percent = Extreme_Events/Total_Words*100,
         Quality_Percent = Quality/Total_Words*100)

write.csv(word_data_drouggtplans,"~/word_data_droughtplans.csv")


```
